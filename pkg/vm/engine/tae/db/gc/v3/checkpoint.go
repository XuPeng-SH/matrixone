// Copyright 2021 Matrix Origin
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//      http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

package gc

import (
	"context"
	"fmt"
	"sort"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/matrixorigin/matrixone/pkg/common/moerr"
	"github.com/matrixorigin/matrixone/pkg/common/mpool"
	"github.com/matrixorigin/matrixone/pkg/container/types"
	"github.com/matrixorigin/matrixone/pkg/fileservice"
	"github.com/matrixorigin/matrixone/pkg/logutil"
	"github.com/matrixorigin/matrixone/pkg/objectio"
	"github.com/matrixorigin/matrixone/pkg/vm/engine/tae/blockio"
	"github.com/matrixorigin/matrixone/pkg/vm/engine/tae/common"
	"github.com/matrixorigin/matrixone/pkg/vm/engine/tae/containers"
	"github.com/matrixorigin/matrixone/pkg/vm/engine/tae/db/checkpoint"
	"github.com/matrixorigin/matrixone/pkg/vm/engine/tae/logtail"
	"github.com/matrixorigin/matrixone/pkg/vm/engine/tae/testutils"
	"go.uber.org/zap"
)

type checkpointCleaner struct {
	ctx context.Context

	// TODO: remove `sid`
	sid string

	mp *mpool.MPool
	fs *objectio.ObjectFS

	// checkpointCli is used to get the instance of the specified checkpoint
	checkpointCli checkpoint.RunnerReader

	watermarks struct {
		// scanWaterMark is the watermark of the incremental checkpoint which has been
		// scanned by the cleaner. After the cleaner scans the checkpoint, it
		// records all the object-list found in the checkpoint into some GC-specific
		// files. The scanWaterMark is used to record the end of the checkpoint.
		// For example:
		// Incremental checkpoint: [t100, t200), [t200, t300), [t300, t400)
		// scanWaterMark: [t100, t200)
		// remainingObjects: windows: [t100, t200), [f1, f2, f3]
		// The cleaner will scan the checkpoint [t200, t300) next time. Then:
		// scanWaterMark: [t100, t200), [t200, t300)
		// remainingObjects: windows:
		// {[t100, t200), [f1, f2, f3]}, {[t200, t300), [f4, f5, f6]}
		scanWaterMark atomic.Pointer[checkpoint.CheckpointEntry]

		// gcWaterMark is the watermark of the global checkpoint which has been GC'ed.
		// when the cleaner GCs the global checkpoint, it scans all the object-list
		// of the global checkpoint and build a bloom filter for the object-list.
		// Then it scans all the remainingObjects generated by `incremental-checkpoint-scan`
		// and filters out the objects that are not in the bloom filter: canGC and cannotGC.
		// it records the cannotGC objects into some GC-specific files and do fine filter on
		// the canGC objects. The gcWaterMark is used to record the end of the global checkpoint
		// when the cleaner finishes this GC operation.
		gcWaterMark atomic.Pointer[checkpoint.CheckpointEntry]

		// checkpointGCWaterMark is the watermark after being merged. The checkpoint runner will
		// GC the entry in the checkpoint tree according to this watermark And the diskcleaner
		// will use this watermark as the start, and use `gcWaterMark` as the end to call `ICKPRange`,
		// to get the ickp to perform a merge operation
		checkpointGCWaterMark atomic.Pointer[types.TS]
	}

	options struct {
		gcEnabled    atomic.Bool
		checkEnabled atomic.Bool
	}

	config struct {
		// minMergeCount is the configuration of the merge GC metadata file.
		// When the GC file is greater than or equal to minMergeCount,
		// the merge GC metadata file will be triggered and the expired file will be deleted.
		minMergeCount atomic.Int64

		canGCCacheSize int
	}

	// remainingObjects is to record the currently valid GCWindow
	remainingObjects struct {
		sync.RWMutex
		window *GCWindow
	}

	// gcState is used to record state of the executed GCs
	gcState struct {
		sync.RWMutex
		filesGCed []string
	}

	// checker is to check whether the checkpoint can be consumed
	checker struct {
		sync.RWMutex
		extras map[string]func(item any) bool
	}

	// delWorker is a worker that deletes s3â€˜s objects or local
	// files, and only one worker will run
	delWorker *GCWorker

	metaFiles map[string]GCMetaFile

	snapshotMeta *logtail.SnapshotMeta
}

func WithCanGCCacheSize(
	size int,
) CheckpointCleanerOption {
	return func(e *checkpointCleaner) {
		e.config.canGCCacheSize = size
	}
}

func WithCheckOption(enable bool) CheckpointCleanerOption {
	return func(e *checkpointCleaner) {
		e.options.checkEnabled.Store(enable)
	}
}

type CheckpointCleanerOption func(*checkpointCleaner)

func NewCheckpointCleaner(
	ctx context.Context,
	sid string,
	fs *objectio.ObjectFS,
	checkpointCli checkpoint.RunnerReader,
	opts ...CheckpointCleanerOption,
) Cleaner {
	cleaner := &checkpointCleaner{
		ctx:           ctx,
		sid:           sid,
		fs:            fs,
		checkpointCli: checkpointCli,
	}
	for _, opt := range opts {
		opt(cleaner)
	}
	cleaner.delWorker = NewGCWorker(fs, cleaner)
	cleaner.config.minMergeCount.Store(MinMergeCount)
	cleaner.snapshotMeta = logtail.NewSnapshotMeta()
	cleaner.options.gcEnabled.Store(true)
	cleaner.mp = common.CheckpointAllocator
	cleaner.checker.extras = make(map[string]func(item any) bool)
	cleaner.metaFiles = make(map[string]GCMetaFile)
	return cleaner
}

func (c *checkpointCleaner) Stop() {
}

func (c *checkpointCleaner) GetMPool() *mpool.MPool {
	return c.mp
}

func (c *checkpointCleaner) SetTid(tid uint64) {
	c.snapshotMeta.Lock()
	defer c.snapshotMeta.Unlock()
	c.snapshotMeta.SetTid(tid)
}

func (c *checkpointCleaner) EnableGC() {
	c.options.gcEnabled.Store(true)
}

func (c *checkpointCleaner) DisableGC() {
	c.options.gcEnabled.Store(false)
}

func (c *checkpointCleaner) GCEnabled() bool {
	return c.options.gcEnabled.Load()
}

func (c *checkpointCleaner) EnableCheck() {
	c.options.checkEnabled.Store(true)
}
func (c *checkpointCleaner) DisableCheck() {
	c.options.checkEnabled.Store(false)
}

func (c *checkpointCleaner) CheckEnabled() bool {
	return c.options.checkEnabled.Load()
}

func (c *checkpointCleaner) Replay() error {
	dirs, err := c.fs.ListDir(GCMetaDir)
	if err != nil {
		return err
	}
	if len(dirs) == 0 {
		return nil
	}

	maxConsumedStart := types.TS{}
	maxConsumedEnd := types.TS{}
	maxSnapEnd := types.TS{}
	maxAcctEnd := types.TS{}
	// Get effective minMerged
	var snapFile, acctFile string
	for _, dir := range dirs {
		start, end, ext := blockio.DecodeGCMetadataFileName(dir.Name)
		if ext == blockio.SnapshotExt && maxSnapEnd.LT(&end) {
			maxSnapEnd = end
			snapFile = dir.Name
		}
		if ext == blockio.AcctExt && maxAcctEnd.LT(&end) {
			maxAcctEnd = end
			acctFile = dir.Name
		}
		c.metaFiles[dir.Name] = GCMetaFile{
			name:  dir.Name,
			start: start,
			end:   end,
			ext:   ext,
		}
	}
	readDirs := make([]fileservice.DirEntry, 0)
	for _, dir := range dirs {
		start, end, ext := blockio.DecodeGCMetadataFileName(dir.Name)
		if ext == blockio.SnapshotExt || ext == blockio.AcctExt {
			continue
		}
		if maxConsumedStart.IsEmpty() || maxConsumedStart.LT(&end) {
			maxConsumedStart = start
			maxConsumedEnd = end
			readDirs = append(readDirs, dir)
		}
	}
	if len(readDirs) == 0 {
		return nil
	}
	logger := logutil.Info
	for _, dir := range readDirs {
		start := time.Now()
		window := NewGCWindow(c.mp, c.fs.Service)
		_, end, _ := blockio.DecodeGCMetadataFileName(dir.Name)
		err = window.ReadTable(c.ctx, GCMetaDir+dir.Name, dir.Size, c.fs, end)
		if err != nil {
			logger = logutil.Error
		}
		logger(
			"Replay-GC-Metadata-File",
			zap.String("name", dir.Name),
			zap.Duration("cost", time.Since(start)),
			zap.Error(err),
		)
		if err != nil {
			return err
		}
		c.addGCWindow(window)
	}
	if acctFile != "" {
		err = c.snapshotMeta.ReadTableInfo(c.ctx, GCMetaDir+acctFile, c.fs.Service)
		if err != nil {
			return err
		}
	}
	if snapFile != "" {
		err = c.snapshotMeta.ReadMeta(c.ctx, GCMetaDir+snapFile, c.fs.Service)
		if err != nil {
			return err
		}
	}
	ckp := checkpoint.NewCheckpointEntry(c.sid, maxConsumedStart, maxConsumedEnd, checkpoint.ET_Incremental)
	c.updateScanWaterMark(ckp)
	if acctFile == "" {
		//No account table information, it may be a new cluster or an upgraded cluster,
		//and the table information needs to be initialized from the checkpoint
		scanWaterMark := c.GetScanWaterMark()
		isConsumedGCkp := false
		checkpointEntries, err := checkpoint.ListSnapshotCheckpoint(c.ctx, c.sid, c.fs.Service, scanWaterMark.GetEnd(), 0)
		if err != nil {
			// TODO: why only warn???
			logutil.Warn(
				"Replay-GC-List-Error",
				zap.Error(err),
			)
		}
		if len(checkpointEntries) == 0 {
			return nil
		}
		for _, entry := range checkpointEntries {
			logutil.Infof("load checkpoint: %s, consumedEnd: %s", entry.String(), scanWaterMark.String())
			ckpData, err := c.collectCkpData(entry)
			if err != nil {
				// TODO: why only warn???
				logutil.Warn(
					"Replay-GC-Collect-Error",
					zap.Error(err),
				)
				continue
			}
			if entry.GetType() == checkpoint.ET_Global {
				isConsumedGCkp = true
			}
			c.snapshotMeta.InitTableInfo(c.ctx, c.fs.Service, ckpData, entry.GetStart(), entry.GetEnd())
		}
		if !isConsumedGCkp {
			// The global checkpoint that Specified checkpoint depends on may have been GC,
			// so we need to load a latest global checkpoint
			entry := c.checkpointCli.MaxGlobalCheckpoint()
			if entry == nil {
				logutil.Warn("not found max global checkpoint!")
				return nil
			}
			logutil.Info(
				"Replay-GC-Load-Global-Checkpoint",
				zap.String("max-gloabl", entry.String()),
				zap.String("max-consumed", scanWaterMark.String()),
			)
			ckpData, err := c.collectCkpData(entry)
			if err != nil {
				// TODO: why only warn???
				logutil.Warn(
					"Replay-GC-Collect-Global-Error",
					zap.Error(err),
				)
				return nil
			}
			c.snapshotMeta.InitTableInfo(c.ctx, c.fs.Service, ckpData, entry.GetStart(), entry.GetEnd())
		}
		logutil.Info(
			"Replay-GC-Init-Table-Info",
			zap.String("info", c.snapshotMeta.TableInfoString()),
		)
	}
	return nil

}

func (c *checkpointCleaner) GetCheckpointMetaFiles() map[string]struct{} {
	return c.checkpointCli.GetCheckpointMetaFiles()
}

func (c *checkpointCleaner) updateScanWaterMark(e *checkpoint.CheckpointEntry) {
	c.watermarks.scanWaterMark.Store(e)
}

func (c *checkpointCleaner) updateGCWaterMark(e *checkpoint.CheckpointEntry) {
	c.watermarks.gcWaterMark.Store(e)
}

func (c *checkpointCleaner) updateCheckpointGCWaterMark(ts *types.TS) {
	c.watermarks.checkpointGCWaterMark.Store(ts)
}

func (c *checkpointCleaner) addGCWindow(window *GCWindow) {
	c.remainingObjects.Lock()
	defer c.remainingObjects.Unlock()
	if c.remainingObjects.window == nil {
		c.remainingObjects.window = window
	} else {
		c.remainingObjects.window.Merge(window)
		window.Close()
	}
}

func (c *checkpointCleaner) recordFilesGCed(files []string) {
	c.gcState.Lock()
	defer c.gcState.Unlock()
	c.gcState.filesGCed = append(c.gcState.filesGCed, files...)
}

func (c *checkpointCleaner) GetScanWaterMark() *checkpoint.CheckpointEntry {
	return c.watermarks.scanWaterMark.Load()
}

func (c *checkpointCleaner) GetMinMerged() *checkpoint.CheckpointEntry {
	return c.GetScanWaterMark()
}

func (c *checkpointCleaner) GetGCWaterMark() *checkpoint.CheckpointEntry {
	return c.watermarks.gcWaterMark.Load()
}

func (c *checkpointCleaner) GetCheckpointGCWaterMark() *types.TS {
	return c.watermarks.checkpointGCWaterMark.Load()
}

func (c *checkpointCleaner) GetGCWindow() *GCWindow {
	c.remainingObjects.RLock()
	defer c.remainingObjects.RUnlock()
	return c.remainingObjects.window
}

func (c *checkpointCleaner) GetGCWindowLocked() *GCWindow {
	return c.remainingObjects.window
}

func (c *checkpointCleaner) SetMinMergeCountForTest(count int) {
	c.config.minMergeCount.Store(int64(count))
}

func (c *checkpointCleaner) getMinMergeCount() int {
	return int(c.config.minMergeCount.Load())
}

func (c *checkpointCleaner) OrhpanFilesGCed() []string {
	c.gcState.RLock()
	defer c.gcState.RUnlock()
	filesGCed := c.gcState.filesGCed
	//Empty the array, in order to store the next file list
	c.gcState.filesGCed = make([]string, 0)
	return filesGCed
}

func (c *checkpointCleaner) mergeSnapshotFile(
	metaFiles map[string]GCMetaFile,
) error {
	scanWaterMark := c.GetScanWaterMark()
	if scanWaterMark == nil {
		return nil
	}
	var (
		maxSnapEnd  types.TS
		maxSnapFile string

		maxAcctEnd  types.TS
		maxAcctFile string

		err error
	)

	//metas := make(map[string]struct{})
	// for name := range c.metaFiles {
	// 	metas[name] = struct{}{}
	// }

	doDeleteFileFn := func(
		thisFile string, thisTS *types.TS,
		maxFile string, maxTS *types.TS,
	) (
		newMaxFile string,
		newMaxTS types.TS,
		err error,
	) {
		if maxFile == "" {
			newMaxFile = GCMetaDir + thisFile
			newMaxTS = *thisTS
			logutil.Info(
				"Merging-GC-File-SnapAcct-File",
				zap.String("max-file", newMaxFile),
				zap.String("max-ts", newMaxTS.ToString()),
			)
			delete(c.metaFiles, thisFile)
			return
		}
		if maxTS.LT(thisTS) {
			newMaxFile = GCMetaDir + thisFile
			newMaxTS = *thisTS
			if err = c.fs.Delete(maxFile); err != nil {
				logutil.Errorf("DelFiles failed: %v, max: %v", err.Error(), newMaxTS.ToString())
				return
			}
			logutil.Info(
				"Merging-GC-File-SnapAcct-File",
				zap.String("max-file", newMaxFile),
				zap.String("max-ts", newMaxTS.ToString()),
			)
			delete(c.metaFiles, thisFile)
			return
		}

		// thisTS <= maxTS: this file is expired and should be deleted
		if err = c.fs.Delete(GCMetaDir + thisFile); err != nil {
			logutil.Errorf("DelFiles failed: %v, file: %s, ts: %s", err.Error(), thisFile, thisTS.ToString())
		}
		delete(c.metaFiles, thisFile)

		return
	}

	for _, metaFile := range metaFiles {
		switch metaFile.Ext() {
		case blockio.SnapshotExt:
			if maxSnapFile, maxSnapEnd, err = doDeleteFileFn(
				metaFile.Name(), metaFile.End(), maxSnapFile, &maxSnapEnd,
			); err != nil {
				return err
			}
		case blockio.AcctExt:
			if maxAcctFile, maxAcctEnd, err = doDeleteFileFn(
				metaFile.Name(), metaFile.End(), maxAcctFile, &maxAcctEnd,
			); err != nil {
				return err
			}
		}
	}
	return nil
}

func (c *checkpointCleaner) upgradeGCFiles(
	metaFiles map[string]GCMetaFile,
	window *GCWindow,
) (err error) {
	// get the scan watermark
	scanWaterMark := c.GetScanWaterMark()
	if scanWaterMark == nil {
		panic("scanWaterMark is nil")
	}
	now := time.Now()
	logutil.Info(
		"[DiskCleaner]",
		zap.String("MergeGCFile-Start", scanWaterMark.String()),
	)
	defer func() {
		logutil.Info(
			"[DiskCleaner]",
			zap.String("MergeGCFile-End", scanWaterMark.String()),
			zap.Duration("cost", time.Since(now)),
		)
	}()
	deleteFiles := make([]string, 0)
	for _, metaFile := range metaFiles {
		if (metaFile.Ext() == blockio.CheckpointExt) &&
			(metaFile.EqualRange(&window.tsRange.start, &window.tsRange.end)) {
			deleteFiles = append(deleteFiles, GCMetaDir+metaFile.Name())
			delete(c.metaFiles, metaFile.Name())
		}
	}

	if err = c.fs.DelFiles(c.ctx, deleteFiles); err != nil {
		logutil.Error(
			"Merging-GC-File-Error",
			zap.Error(err),
		)
	}
	return
}

// getMetaFilesToMerge returns the files that can be merged.
// metaFiles: all checkpoint meta files should be merged.
func (c *checkpointCleaner) getMetaFilesToMerge() (
	checkpoints []*checkpoint.CheckpointEntry,
) {
	start := c.GetCheckpointGCWaterMark()
	end := c.GetGCWaterMark().GetEnd()
	if !end.GT(start) {
		return nil
	}
	return c.checkpointCli.ICKPRange(start, &end, 10)
}

func (c *checkpointCleaner) getDeleteFile(
	checkpoints []*checkpoint.CheckpointEntry,
	stage types.TS,
	ckpSnapList []types.TS,
) ([]string, []*checkpoint.CheckpointEntry, error) {
	if len(checkpoints) == 0 {
		return nil, nil, nil
	}
	deleteFiles := make([]string, 0)
	var mergeFiles []*checkpoint.CheckpointEntry
	for i := len(checkpoints) - 1; i >= 0; i-- {
		// TODO: remove this log
		logutil.Info("[MergeCheckpoint]",
			common.OperationField("List Checkpoint"),
			common.OperandField(checkpoints[i].String()))
	}
	for i := len(checkpoints) - 1; i >= 0; i-- {
		ckp := checkpoints[i]
		end := ckp.GetEnd()
		if end.LT(&stage) {
			if isSnapshotCKPRefers(ckp.GetStart(), ckp.GetEnd(), ckpSnapList) {
				// TODO: remove this log
				logutil.Info("[MergeCheckpoint]",
					common.OperationField("isSnapshotCKPRefers"),
					common.OperandField(ckp.String()))
				mergeFiles = checkpoints[:i+1]
				break
			}
			logutil.Info("[MergeCheckpoint]",
				common.OperationField("GC checkpoint"),
				common.OperandField(ckp.String()))
			nameMeta := blockio.EncodeCheckpointMetadataFileName(
				checkpoint.CheckpointDir, checkpoint.PrefixMetadata,
				ckp.GetStart(), ckp.GetEnd())
			locations, err := logtail.LoadCheckpointLocations(
				c.ctx, c.sid, ckp.GetTNLocation(), ckp.GetVersion(), c.fs.Service)
			if err != nil {
				if moerr.IsMoErrCode(err, moerr.ErrFileNotFound) {
					deleteFiles = append(deleteFiles, nameMeta)
					continue
				}
				return nil, nil, err
			}
			deleteFiles = append(deleteFiles, nameMeta)
			for name := range locations {
				deleteFiles = append(deleteFiles, name)
			}
			deleteFiles = append(deleteFiles, ckp.GetTNLocation().Name().String())
		}
	}
	return deleteFiles, mergeFiles, nil
}

func (c *checkpointCleaner) mergeCheckpointFiles(
	checkpointLowWaterMark types.TS,
	accoutSnapshots map[uint32][]types.TS,
	memoryBuffer *containers.OneSchemaBatchBuffer,
) error {
	// checkpointLowWaterMark is empty only in the following cases:
	// 1. no incremental and no gloabl checkpoint
	// 2. one incremental checkpoint with empty start
	// 3. no incremental checkpoint and one or more global checkpoints
	// if there are global checkpoints and incremental checkpoints, the low water mark is:
	// min(min(startTS of all incremental checkpoints),min(endTS of global checkpoints))
	if checkpointLowWaterMark.IsEmpty() {
		return nil
	}
	checkpoints := c.getMetaFilesToMerge()
	logutil.Infof("[MergeCheckpoint] checkpointMetaFiles len %d", len(checkpoints))
	if len(checkpoints) == 0 {
		return nil
	}

	gcWaterMark := c.GetGCWaterMark().GetEnd()
	rangeEnd := checkpoints[len(checkpoints)-1].GetEnd()
	if rangeEnd.LT(&gcWaterMark) {
		panic(fmt.Sprintf("rangeEnd %s < gcWaterMark %s", rangeEnd, gcWaterMark))
	}

	deleteFiles := make([]string, 0)
	ckpSnapList := make([]types.TS, 0)
	for _, ts := range accoutSnapshots {
		ckpSnapList = append(ckpSnapList, ts...)
	}
	sort.Slice(ckpSnapList, func(i, j int) bool {
		return ckpSnapList[i].LT(&ckpSnapList[j])
	})
	logutil.Info("[MergeCheckpoint]",
		common.OperationField("MergeCheckpointFiles"),
		common.OperandField(checkpointLowWaterMark.ToString()))
	dFiles, mergeFiles, err := c.getDeleteFile(
		checkpoints,
		checkpointLowWaterMark,
		ckpSnapList,
	)
	if err != nil {
		return err
	}
	deleteFiles = append(deleteFiles, dFiles...)
	// merge ickp
	var newCheckpoint *checkpoint.CheckpointEntry
	window := c.GetGCWindow()

	if len(mergeFiles) > 0 {
		ckpEnd := mergeFiles[len(mergeFiles)-1].GetEnd()
		end := window.tsRange.end
		if ckpEnd.GT(&end) {
			panic("checkpoint end ts is greater than window end ts")
		}
	}

	sourcer := window.MakeFilesReader(c.ctx, c.fs.Service)
	bf, err := BuildBloomfilter(
		c.ctx,
		Default_Coarse_EstimateRows,
		Default_Coarse_Probility,
		0,
		sourcer.Read,
		memoryBuffer,
		c.mp,
	)
	dFiles, newCheckpoint, err = MergeCheckpoint(
		c.ctx,
		c.sid,
		c.fs.Service,
		mergeFiles,
		bf,
		blockio.EncodeCheckpointMetadataFileName,
		c.mp)
	if err != nil {
		return err
	}
	if newCheckpoint != nil {
		//TODO: add to checkpoint tree
		c.checkpointCli.AddCompacted(newCheckpoint)
		ts := newCheckpoint.GetEnd()

		// update checkpoint gc water mark
		c.updateCheckpointGCWaterMark(&ts)
	}
	deleteFiles = append(deleteFiles, dFiles...)

	// When the compacted tree exceeds 10 entries, you need to merge once
	compacted, ok := c.checkpointCli.CompactedRange(10)
	if ok {
		dFiles, newCheckpoint, err = MergeCheckpoint(c.ctx,
			c.sid,
			c.fs.Service,
			compacted,
			bf,
			blockio.EncodeCompactedMetadataFileName,
			c.mp)
		if err != nil {
			return err
		}
		if newCheckpoint != nil {
			c.checkpointCli.AddCompacted(newCheckpoint)
		}
		deleteFiles = append(deleteFiles, dFiles...)
		for _, ckp := range compacted {
			c.checkpointCli.DeleteCompactedEntry(ckp)
		}
	}

	logutil.Info("[MergeCheckpoint]",
		common.OperationField("CKP GC"),
		common.OperandField(deleteFiles))
	err = c.fs.DelFiles(c.ctx, deleteFiles)
	if err != nil {
		logutil.Errorf("DelFiles failed: %v", err.Error())
		return err
	}
	for _, file := range deleteFiles {
		if strings.Contains(file, checkpoint.PrefixMetadata) {
			info := strings.Split(file, checkpoint.CheckpointDir+"/")
			name := info[1]
			c.checkpointCli.RemoveCheckpointMetaFile(name)
		}
	}
	return nil
}

func (c *checkpointCleaner) collectCkpData(
	ckp *checkpoint.CheckpointEntry,
) (data *logtail.CheckpointData, err error) {
	return logtail.GetCheckpointData(
		c.ctx, c.sid, c.fs.Service, ckp.GetLocation(), ckp.GetVersion())
}

func (c *checkpointCleaner) GetPITRs() (*logtail.PitrInfo, error) {
	ts := time.Now()
	return c.snapshotMeta.GetPITR(c.ctx, c.sid, ts, c.fs.Service, c.mp)
}

func (c *checkpointCleaner) TryGC() (err error) {
	var maxGlobalCKP *checkpoint.CheckpointEntry
	if maxGlobalCKP = c.checkpointCli.MaxGlobalCheckpoint(); maxGlobalCKP == nil {
		return
	}
	memoryBuffer := MakeGCWindowBuffer(16 * mpool.MB)
	defer memoryBuffer.Close(c.mp)
	if err = c.tryGCAgainstGlobalCheckpoint(
		maxGlobalCKP, memoryBuffer,
	); err != nil {
		logutil.Error(
			"DiskCleaner-Replay-TryGC-Error",
			zap.Error(err),
			zap.String("checkpoint", maxGlobalCKP.String()),
		)
		return
	}
	var window *GCWindow
	if window = c.GetGCWindow(); window == nil {
		return
	}
	metaFiles := make(map[string]GCMetaFile, len(c.metaFiles))
	for k, v := range c.metaFiles {
		metaFiles[k] = v
	}
	if err = c.upgradeGCFiles(
		metaFiles, window,
	); err != nil {
		logutil.Error(
			"DiskCleaner-Replay-UpgradeGC-Error",
			zap.Error(err),
		)
	}
	return
}

func (c *checkpointCleaner) tryGCAgainstGlobalCheckpoint(
	gckp *checkpoint.CheckpointEntry,
	memoryBuffer *containers.OneSchemaBatchBuffer,
) (err error) {
	if !c.delWorker.Start() {
		return
	}
	var snapshots map[uint32]containers.Vector
	defer func() {
		if err != nil {
			logutil.Errorf("[DiskCleaner] tryGCAgainstGlobalCheckpoint failed: %v", err.Error())
			c.delWorker.Idle()
		}
		logtail.CloseSnapshotList(snapshots)
	}()
	pitrs, err := c.GetPITRs()
	if err != nil {
		logutil.Errorf("[DiskCleaner] GetPitrs failed: %v", err.Error())
		return
	}
	snapshots, err = c.snapshotMeta.GetSnapshot(c.ctx, c.sid, c.fs.Service, c.mp)
	if err != nil {
		logutil.Errorf("[DiskCleaner] GetSnapshot failed: %v", err.Error())
		return
	}
	accountSnapshots := TransformToTSList(snapshots)
	filesToGC, err := c.doGCAgainstGlobalCheckpoint(
		gckp, accountSnapshots, pitrs, memoryBuffer,
	)
	if err != nil {
		logutil.Errorf("[DiskCleaner] doGCAgainstGlobalCheckpoint failed: %v", err.Error())
		return
	}
	// Delete files after doGCAgainstGlobalCheckpoint
	// TODO:Requires Physical Removal Policy
	if err = c.delWorker.ExecDelete(c.ctx, filesToGC); err != nil {
		logutil.Infof("[DiskCleaner] ExecDelete failed: %v", err)
		return
	}
	if err = c.mergeCheckpointFiles(
		c.checkpointCli.GetLowWaterMark(), accountSnapshots, memoryBuffer,
	); err != nil {
		// TODO: Error handle
		logutil.Errorf("[DiskCleaner] mergeCheckpointFiles failed: %v", err.Error())
	}
	return
}

func (c *checkpointCleaner) doGCAgainstGlobalCheckpoint(
	gckp *checkpoint.CheckpointEntry,
	accountSnapshots map[uint32][]types.TS,
	pitrs *logtail.PitrInfo,
	memoryBuffer *containers.OneSchemaBatchBuffer,
) ([]string, error) {
	c.remainingObjects.Lock()
	defer c.remainingObjects.Unlock()

	now := time.Now()

	var softCost, mergeCost time.Duration
	defer func() {
		logutil.Info("[DiskCleaner] doGCAgainstGlobalCheckpoint cost",
			zap.String("soft-gc cost", softCost.String()),
			zap.String("merge-table cost", mergeCost.String()))
	}()

	window := c.GetGCWindowLocked()
	if window == nil {
		return nil, nil
	}

	var (
		filesToGC []string
		metafile  string
		err       error
	)
	// do GC against the global checkpoint
	// the result is the files that need to be deleted
	// it will update the file list in the window
	// Before:
	// [t100, t400] [f1, f2, f3, f4, f5, f6, f7, f8, f9]
	// After:
	// [t100, t400] [f10, f11]
	// Also, it will update the GC metadata
	if filesToGC, metafile, err = window.ExecuteGlobalCheckpointBasedGC(
		c.ctx,
		gckp,
		accountSnapshots,
		pitrs,
		c.snapshotMeta,
		memoryBuffer,
		c.config.canGCCacheSize,
		c.mp,
		c.fs.Service,
	); err != nil {
		logutil.Errorf("doGCAgainstGlobalCheckpoint failed: %v", err.Error())
		return nil, err
	}
	c.metaFiles[metafile] = GCMetaFile{
		name:  metafile,
		start: window.tsRange.start,
		end:   window.tsRange.end,
		ext:   blockio.CheckpointExt,
	}

	softCost = time.Since(now)

	// update gc watermark and refresh snapshot meta with the latest gc result
	// gcWaterMark will be updated to the end of the global checkpoint after each GC
	// Before:
	// gcWaterMark: GCKP[t100, t200)
	// After:
	// gcWaterMark: GCKP[t200, t400)
	now = time.Now()
	// TODO:
	c.updateGCWaterMark(gckp)
	c.snapshotMeta.MergeTableInfo(accountSnapshots, pitrs)
	mergeCost = time.Since(now)
	return filesToGC, nil
}

func (c *checkpointCleaner) scanCheckpointsAsDebugWindow(
	ckps []*checkpoint.CheckpointEntry,
	buffer *containers.OneSchemaBatchBuffer,
) (window *GCWindow, err error) {
	window = NewGCWindow(c.mp, c.fs.Service, WithMetaPrefix("debug/"))
	if _, err = window.ScanCheckpoints(
		c.ctx, ckps, c.collectCkpData, nil, nil, buffer,
	); err != nil {
		window.Close()
		window = nil
	}
	return
}

func (c *checkpointCleaner) DoCheck() error {
	debugCandidates := c.checkpointCli.GetAllIncrementalCheckpoints()

	c.remainingObjects.RLock()
	defer c.remainingObjects.RUnlock()

	// no scan watermark, GC has not yet run
	var scanWaterMark *checkpoint.CheckpointEntry
	if scanWaterMark = c.GetScanWaterMark(); scanWaterMark == nil {
		return moerr.NewInternalErrorNoCtx("GC has not yet run")
	}

	gCkp := c.GetGCWaterMark()
	testutils.WaitExpect(10000, func() bool {
		gCkp = c.GetGCWaterMark()
		return gCkp != nil
	})
	if gCkp == nil {
		gCkp = c.checkpointCli.MaxGlobalCheckpoint()
		if gCkp == nil {
			return nil
		}
		logutil.Warnf("MaxCompared is nil, use maxGlobalCkp %v", gCkp.String())
	}
	for i, ckp := range debugCandidates {
		maxEnd := scanWaterMark.GetEnd()
		ckpEnd := ckp.GetEnd()
		if ckpEnd.Equal(&maxEnd) {
			debugCandidates = debugCandidates[:i+1]
			break
		}
	}
	start1 := debugCandidates[len(debugCandidates)-1].GetEnd()
	start2 := scanWaterMark.GetEnd()
	if !start1.Equal(&start2) {
		logutil.Info("[DiskCleaner]", common.OperationField("Compare not equal"),
			common.OperandField(start1.ToString()), common.OperandField(start2.ToString()))
		return moerr.NewInternalErrorNoCtx("TS Compare not equal")
	}

	buffer := MakeGCWindowBuffer(16 * mpool.MB)
	defer buffer.Close(c.mp)

	debugWindow, err := c.scanCheckpointsAsDebugWindow(
		debugCandidates, buffer,
	)
	if err != nil {
		logutil.Errorf("processing clean %s: %v", debugCandidates[0].String(), err)
		// TODO
		return moerr.NewInternalErrorNoCtxf("processing clean %s: %v", debugCandidates[0].String(), err)
	}

	snapshots, err := c.GetSnapshots()
	if err != nil {
		logutil.Errorf("processing clean %s: %v", debugCandidates[0].String(), err)
		return moerr.NewInternalErrorNoCtxf("processing clean GetSnapshots %s: %v", debugCandidates[0].String(), err)
	}
	defer logtail.CloseSnapshotList(snapshots)

	pitr, err := c.GetPITRs()
	if err != nil {
		logutil.Errorf("processing clean %s: %v", debugCandidates[0].String(), err)
		return moerr.NewInternalErrorNoCtxf("processing clean GetPITRs %s: %v", debugCandidates[0].String(), err)
	}

	var window GCWindow
	if w := c.GetGCWindowLocked(); w != nil {
		window = w.Clone()
	}
	defer window.Close()

	accoutSnapshots := TransformToTSList(snapshots)
	logutil.Infof(
		"merge table is %d, stats is %v",
		len(window.files),
		window.files[0].ObjectName().String(),
	)
	if _, _, err = window.ExecuteGlobalCheckpointBasedGC(
		c.ctx,
		gCkp,
		accoutSnapshots,
		pitr,
		c.snapshotMeta,
		buffer,
		c.config.canGCCacheSize,
		c.mp,
		c.fs.Service,
	); err != nil {
		logutil.Infof("err is %v", err)
		return err
	}

	logutil.Infof(
		"merge table2 is %d, stats is %v",
		len(window.files),
		window.files[0].ObjectName().String(),
	)

	//logutil.Infof("debug table is %d, stats is %v", len(debugWindow.files.stats), debugWindow.files.stats[0].ObjectName().String())
	if _, _, err = debugWindow.ExecuteGlobalCheckpointBasedGC(
		c.ctx,
		gCkp,
		accoutSnapshots,
		pitr,
		c.snapshotMeta,
		buffer,
		c.config.canGCCacheSize,
		c.mp,
		c.fs.Service,
	); err != nil {
		logutil.Infof("err is %v", err)
		return err
	}

	//logutil.Infof("debug table2 is %d, stats is %v", len(debugWindow.files.stats), debugWindow.files.stats[0].ObjectName().String())
	objects1, objects2, equal := window.Compare(debugWindow, buffer)
	if !equal {
		logutil.Errorf("remainingObjects :%v", window.String(objects1))
		logutil.Errorf("debugWindow :%v", debugWindow.String(objects2))
		return moerr.NewInternalErrorNoCtx("Compare is failed")
	} else {
		logutil.Info("[DiskCleaner]", common.OperationField("Compare is End"),
			common.AnyField("table :", debugWindow.String(objects2)),
			common.OperandField(start1.ToString()))
	}
	return nil
}

func (c *checkpointCleaner) Process() {
	if !c.GCEnabled() {
		return
	}

	now := time.Now()
	defer func() {
		logutil.Info(
			"DiskCleaner-Process-End",
			zap.Duration("duration", time.Since(now)),
		)
	}()

	// 1. get the max consumed timestamp water level
	var tsWaterMark types.TS
	if scanWaterMark := c.GetScanWaterMark(); scanWaterMark != nil {
		tsWaterMark = scanWaterMark.GetEnd()
	}

	// 2. get up to 10 incremental checkpoints starting from the tsWaterMark
	checkpoints := c.checkpointCli.ICKPSeekLT(tsWaterMark, 10)
	// if there is no incremental checkpoint, quickly return
	if len(checkpoints) == 0 {
		logutil.Info(
			"DiskCleaner-Process-NoCheckpoint",
			zap.String("water-level", tsWaterMark.ToString()),
		)
		return
	}

	// 3. filter out the incremental checkpoints that do not meet the requirements
	candidates := make([]*checkpoint.CheckpointEntry, 0, len(checkpoints))
	for _, ckp := range checkpoints {
		// TODO:
		// 1. break or continue?
		// 2. use cases reveiw!
		if !c.checkExtras(ckp) {
			break
		}
		candidates = append(candidates, ckp)
	}
	if len(candidates) == 0 {
		return
	}

	var (
		gcWindow *GCWindow
		err      error
	)
	memoryBuffer := MakeGCWindowBuffer(16 * mpool.MB)
	defer memoryBuffer.Close(c.mp)
	if gcWindow, err = c.scanCheckpointsAsOneWindow(
		candidates, memoryBuffer,
	); err != nil {
		logutil.Error(
			"DiskCleaner-Process-CreateNewInput-Error",
			zap.Error(err),
			zap.String("checkpoint", candidates[0].String()),
		)
		// TODO
		return
	}
	c.addGCWindow(gcWindow)
	c.updateScanWaterMark(candidates[len(candidates)-1])

	// get the max global checkpoint
	// if there is no global checkpoint, quickly return
	var maxGlobalCKP *checkpoint.CheckpointEntry
	if maxGlobalCKP = c.checkpointCli.MaxGlobalCheckpoint(); maxGlobalCKP == nil {
		return
	}

	// get the gc watermark, which is the end of the global checkpoint after last GC
	var gcWaterMarkTS types.TS
	if gcWaterMark := c.GetGCWaterMark(); gcWaterMark != nil {
		gcWaterMarkTS = gcWaterMark.GetEnd()
	}

	// if the gc watermark is less than the end of the global checkpoint, it means
	// that the maxGlobalCKP has not been GCed, and we need to GC it now
	nextGCTS := maxGlobalCKP.GetEnd()
	metaFiles := make(map[string]GCMetaFile, len(c.metaFiles))
	for k, v := range c.metaFiles {
		metaFiles[k] = v
	}
	if gcWaterMarkTS.LT(&nextGCTS) {
		logutil.Info(
			"DiskCleaner-Process-TryGC",
			zap.String("max-global", maxGlobalCKP.String()),
			zap.String("gc-watermark", gcWaterMarkTS.ToString()),
		)
		if err = c.tryGCAgainstGlobalCheckpoint(
			maxGlobalCKP, memoryBuffer,
		); err != nil {
			logutil.Error(
				"DiskCleaner-Process-TryGC-Error",
				zap.Error(err),
				zap.String("checkpoint", maxGlobalCKP.String()),
			)
			return
		}

		if err = c.upgradeGCFiles(
			metaFiles, c.GetGCWindow(),
		); err != nil {
			logutil.Error(
				"DiskCleaner-Process-UpgradeGC-Error",
				zap.Error(err),
			)
			return
		}
	}

	if err = c.mergeSnapshotFile(metaFiles); err != nil {
		// TODO: Error handle
		return
	}

	if !c.CheckEnabled() {
		return
	}
}

func (c *checkpointCleaner) checkExtras(item any) bool {
	c.checker.RLock()
	defer c.checker.RUnlock()
	for _, checker := range c.checker.extras {
		if !checker(item) {
			return false
		}
	}
	return true
}

// AddChecker add&update a checker to the cleanerï¼Œreturn the number of checkers
// key is the unique identifier of the checker
func (c *checkpointCleaner) AddChecker(checker func(item any) bool, key string) int {
	c.checker.Lock()
	defer c.checker.Unlock()
	c.checker.extras[key] = checker
	return len(c.checker.extras)
}

// RemoveChecker remove a checker from the cleanerï¼Œreturn true if the checker is removed successfully
func (c *checkpointCleaner) RemoveChecker(key string) error {
	c.checker.Lock()
	defer c.checker.Unlock()
	if len(c.checker.extras) == 1 {
		return moerr.NewCantDelGCCheckerNoCtx()
	}
	delete(c.checker.extras, key)
	return nil
}

func (c *checkpointCleaner) scanCheckpointsAsOneWindow(
	ckps []*checkpoint.CheckpointEntry,
	memoryBuffer *containers.OneSchemaBatchBuffer,
) (gcWindow *GCWindow, err error) {
	now := time.Now()
	logutil.Info(
		"DiskCleaner-Consume-Start",
		zap.Int("entry-count :", len(ckps)),
	)

	var (
		snapSize, tableSize uint32
	)
	defer func() {
		logutil.Info("DiskCleaner-Consume-End",
			zap.Duration("duration", time.Since(now)),
			zap.Uint32("snap-meta-size :", snapSize),
			zap.Uint32("table-meta-size :", tableSize),
			zap.String("snapshot-detail", c.snapshotMeta.String()))
	}()

	var snapshotFile, accountFile GCMetaFile
	saveSnapshot := func() (err2 error) {
		name := blockio.EncodeSnapshotMetadataFileName(
			PrefixSnapMeta,
			ckps[0].GetStart(),
			ckps[len(ckps)-1].GetEnd(),
		)
		if snapSize, err2 = c.snapshotMeta.SaveMeta(
			GCMetaDir+name, c.fs.Service,
		); err2 != nil {
			logutil.Error(
				"DiskCleaner-Error-SaveMeta",
				zap.Error(err2),
			)
			return
		}
		snapshotFile = GCMetaFile{
			name:  name,
			start: ckps[0].GetStart(),
			end:   ckps[len(ckps)-1].GetEnd(),
			ext:   blockio.SnapshotExt,
		}
		name = blockio.EncodeTableMetadataFileName(
			PrefixAcctMeta,
			ckps[0].GetStart(),
			ckps[len(ckps)-1].GetEnd(),
		)
		if tableSize, err2 = c.snapshotMeta.SaveTableInfo(
			GCMetaDir+name, c.fs.Service,
		); err2 != nil {
			logutil.Error(
				"DiskCleaner-Error-SaveTableInfo",
				zap.Error(err2),
			)
		}
		accountFile = GCMetaFile{
			name:  name,
			start: ckps[0].GetStart(),
			end:   ckps[len(ckps)-1].GetEnd(),
			ext:   blockio.AcctExt,
		}
		return
	}

	gcWindow = NewGCWindow(c.mp, c.fs.Service)
	var gcMetaFile string
	if gcMetaFile, err = gcWindow.ScanCheckpoints(
		c.ctx,
		ckps,
		c.collectCkpData,
		c.updateSnapshot,
		saveSnapshot,
		memoryBuffer,
	); err != nil {
		gcWindow.Close()
		gcWindow = nil
	}

	c.metaFiles[snapshotFile.name] = snapshotFile
	c.metaFiles[accountFile.name] = accountFile
	c.metaFiles[gcMetaFile] = GCMetaFile{
		name:  gcMetaFile,
		start: gcWindow.tsRange.start,
		end:   gcWindow.tsRange.end,
		ext:   blockio.CheckpointExt,
	}
	return
}

func (c *checkpointCleaner) updateSnapshot(
	ckp *checkpoint.CheckpointEntry,
	data *logtail.CheckpointData,
) error {
	c.snapshotMeta.Update(c.ctx, c.fs.Service, data, ckp.GetStart(), ckp.GetEnd())
	return nil
}

func (c *checkpointCleaner) GetSnapshots() (map[uint32]containers.Vector, error) {
	return c.snapshotMeta.GetSnapshot(c.ctx, c.sid, c.fs.Service, c.mp)
}

func isSnapshotCKPRefers(start, end types.TS, snapVec []types.TS) bool {
	if len(snapVec) == 0 {
		return false
	}
	left, right := 0, len(snapVec)-1
	for left <= right {
		mid := left + (right-left)/2
		snapTS := snapVec[mid]
		if snapTS.GE(&start) && snapTS.LT(&end) {
			logutil.Debugf("isSnapshotRefers: %s, create %v, drop %v",
				snapTS.ToString(), start.ToString(), end.ToString())
			return true
		} else if snapTS.LT(&start) {
			left = mid + 1
		} else {
			right = mid - 1
		}
	}
	return false
}
